id,authors,title,abstract,accepted,image_path
B1tMhcIDM,Chen Brestel-Ran Shadmi-Itamar Tamir-Michal Cohen-Sfaty-Eldad Elnekave,RadBot-CXR: Classification of Four Clinical Finding Categories in Chest X-Ray Using Deep Learning,"The well-documented global shortage of radiologists is most acutely manifested in countries where the rapid rise of a middle class has created a new capacity to produce imaging studies at a rate which far exceeds the time required to train experts capable of interpreting such studies. The production to interpretation gap is seen clearly in the case of the most common of imaging studies: the chest x-ray, where technicians are increasingly called upon to not only acquire the image, but also to interpret it. The dearth of expert radiologists leads to both delayed and inaccurate diagnostic insights.  The present study utilizes a robust radiology database, machine-learning technologies, and robust clinical validation to produce expert-level automatic interpretation of routine chest x-rays.  Using a convolutional neural network (CNN) we achieve  
a performance which is slightly higher than radiologists in the detection of four common chest X-ray (CXR) findings which include focal lung opacities, diffuse lung opacity, cardiomegaly, and abnormal hilar prominence. The agreement of \algoname \space vs. radiologists is slightly higher (1-7\%) than the agreement among a team of three expert radiologists. 
",True,images/B1tMhcIDM
BkuVie2iM,Priya Lakshmi Narayanan-Andrew Dodson-Barry Gusterson-Mitchell Dowsett-Yinyin Yuan,DeepSDCS: Dissecting cancer proliferation heterogeneity in Ki67 digital whole slide images,"Ki67 is an important biomarker for breast cancer. Classification of positive and negative Ki67 cells in histology slides is a common approach to determine cancer proliferation status. However, there is a lack of generalizable and accurate methods to automate Ki67 scoring in large-scale patient cohorts. In this work, we have employed a novel deep learning technique based on hypercolumn descriptors for cell classification in Ki67 images. Specifically, we developed the Simultaneous Detection and Cell Segmentation (DeepSDCS) network to perform cell segmentation and detection. VGG16 network was used for the training and fine tuning to training data. We extracted the hypercolumn descriptors of each cell to form the vector of activation from specific layers to capture features at different granularity. Features from these layers that correspond to the same pixel were propagated using a stochastic gradient descent optimizer to yield the detection of the nuclei and the final cell segmentations. Subsequently, seeds generated from cell segmentation were propagated to a spatially constrained convolutional neural network for the classification of the cells into stromal, lymphocyte, Ki67-positive cancer cell, and Ki67-negative cancer cell. Cells were subsequently classified using a spatially constrained network. We validated its accuracy in the context of a large-scale clinical trial of oestrogen-receptor-positive breast cancer. We achieved 99.06% and 89.59% accuracy on two separate test sets of Ki67 stained breast cancer dataset comprising biopsy and whole-slide images.",True,images/BkuVie2iM
rk5UYassf,Zara Alaverdyan-Julien Jung-Romain Bouet-Carole Lartizien,Regularized siamese neural network for unsupervised outlier detection on brain multiparametric magnetic resonance imaging: application to epilepsy lesion screening,"Computer aided diagnosis (CAD) systems are designed to assist clinicians in various
tasks, including highlighting abnormal regions in medical images. Common
methods exploit supervised learning using annotated data sets and perform classification
at voxel-level. However, many pathologies are characterized by subtle
lesions that may be located anywhere in the organ of interest, have various shapes,
sizes and textures. Acquiring a data set adequately representing the heterogeneity
of such pathologies is therefore a major issue. Moreover, when a lesion is not
visually detected on a scan, outlining it accurately is not feasible. Performing
supervised learning on such labeled data would not be reliable. In this study, we
consider the problem of detecting subtle epilepsy lesions in multiparametric (T1w,
FLAIR) MRI exams considered as normal (MRI-negative). We cast this problem
as an outlier detection problem and build on a previously proposed approach that
consists in learning a oc-SVM model for each voxel in the brain volume using a
small number of clinically-guided features. Our goal in this study is to make
a step forward by replacing the handcrafted features with automatically learnt
representations using neural networks. We propose a novel version of siamese networks
trained on patches extracted from healthy patients’ scans only. This network,
composed of stacked convolutional autoencoders as subnetworks, is regularized by
the reconstruction error of the patches. It is designed to map patches centered at
the same spatial localization to ’close’ representations with respect to the chosen
metric (i.e. cosine) in a latent space. Finally, the middle layer representations of
the subnetworks are fed into oc-SVM models at voxel-level. The model is trained on 
75 healthy subjects and validated on 21 patients with confirmed epilepsy lesions 
(with 18 MR negative patients) and shows a promising performance.",True,images/rk5UYassf
BkZu9wooz,Mattias P. Heinrich-Ozan Oktay-Nassim Bouteldja,OBELISK - One Kernel to Solve Nearly Everything: Unified 3D Binary Convolutions for Image Analysis,"Deep networks have set the state-of-the-art in most image analysis tasks by replacing handcrafted features with learned convolution filters within end-to-end trainable architectures. Still, the specifications of a convolutional network are subject to much manual design - the shape and size of the receptive field for convolutional operations is a very sensitive part that has to be tuned for different image analysis applications. 3D fully-convolutional multi-scale architectures with skip-connection that excel at semantic segmentation and landmark localisation have huge memory requirements and rely on large annotated datasets - an important limitation for wider adaptation in medical image analysis. 

We propose a novel and effective method based on a single trainable 3D convolution kernel that addresses these issues and enables high quality results with a compact four-layer architecture and without sensitive hyperparameters for convolutions and architectural design. Instead of a manual choice of filter size, dilation of weights, and number of scales, our one binary extremely large and inflecting sparse kernel (OBELISK) automatically learns filter offsets in a differentiable continuous space together with weight coefficients. Geometric data augmentation can be directly incorporated into the training by simple coordinate transforms. This powerful new architecture has less than 130'000 parameters, can be trained in few minutes with only 700 MBytes of memory and achieves an increase of Dice overlap of +5.5\% compared to the U-Net for CT multi-organ segmentation.
",True,images/BkZu9wooz
BkIBHb2sG,Hoel Kervadec-Jose Dolz-Meng Tang-Eric Granger-Yuri Boykov-Ismail Ben Ayed,Size-constraint loss for weakly supervised CNN segmentation,"Weak supervision, e.g., in the form of partial labels or image tags, is currently attracting significant attention in CNN segmentation as it can mitigate the lack of full and laborious pixel/voxel annotations, a common problem in medical imaging. Embedding high-order (global) inequality constraints on the network output, for instance, on the size of the target region, can leverage unlabeled data, guiding training with domain-specific knowledge. Inequality constraints are very flexible because they do not assume exact prior knowledge. However, constrained Lagrangian optimization has been largely avoided in deep networks, mainly for computational tractability reasons. To the best of our knowledge, the method of Pathak et al. is the only prior work that addresses constrained deep CNNs in weakly supervised segmentation. It uses the constraints to synthesize fully-labeled training masks (proposals) from weak labels, mimicking full supervision and facilitating dual optimization.
 
We propose to introduce a differentiable term, which enforces inequality constraints directly in the loss function, avoiding expensive Lagrangian dual iterates and proposal generation. From constrained-optimization perspective, our simple approach is not optimal as there is no guarantee that the constraints are satisfied. However, surprisingly, it yields substantially better results than the proposal-based constrained CNNs in Pathak et al., while reducing the computational demand for training. In the context of cardiac image segmentation, we reached a segmentation performance close to full supervision while using a fraction of the ground-truth labels 0.1% of the pixels of the ground-truth masks) and image-level tags. Our framework can be easily extended to other inequality constraints, e.g., shape moments or region statistics. Therefore, it has the potential to close the gap between weakly and fully supervised learning in semantic medical image segmentation. Our code is publicly available.",True,images/BkIBHb2sG
SyQK4-nsz,Amir Alansary-Ozan Oktay-Yuanwei Li-Loic Le Folgoc-Benjamin Hou-Ghislain Vaillant-Ben Glocker-Bernhard Kainz-Daniel Rueckert,Evaluating Reinforcement Learning Agents for Anatomical Landmark Detection,"Automatic detection of anatomical landmarks is an important step for a wide range of applications in medical image analysis. Manual annotation of such landmarks is a tedious task and prone to observer errors. In this paper, we evaluate novel deep Reinforcement Learning (RL) strategies to train agents that can precisely localize target landmarks in medical scans.  An artificial RL agent learns to identify the optimal path to the point of interest by interacting with an environment, in our case 3D images.  Furthermore, we investigate the use of fixed- and multi-scale search strategies with hierarchical action steps in a coarse-to-fine manner. Multiple Deep Q-Network (DQN) based architectures are experimented in training of the proposed RL agents achieving good results for detecting multiple landmarks using a challenging fetal head ultrasound dataset.",True,images/SyQK4-nsz
B1L4Gb3sf,Rodney LaLonde-Ulas Bagci,Capsules for Object Segmentation,"Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al., referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutional-deconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures.  SegCaps is able to handle large image sizes (512 x 512) as opposed to baseline capsules (typically less than 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4% while still providing a better segmentation accuracy. ",True,images/B1L4Gb3sf
B1AxE-njf,Mauricio Orbes-Arteaga-Lauge Sørensen-Marc Modat-M. Jorge Cardoso-Sébastien Ourselin-Mads Nielsen-Akshay Pai.,Simultaneous synthesis of FLAIR and segmentation of white matter hypointensities from T1 MRIs,"Segmenting vascular pathologies such as white matter lesions in Brain magnetic resonance images (MRIs) require acquisition of multiple sequences such as T1-weighted (T1-w) --on which lesions appear hypointense-- and  fluid attenuated inversion recovery (FLAIR) sequence --where lesions appear hyperintense--. However, most of the existing retrospective datasets do not consist of FLAIR sequences. Existing missing modality imputation methods separate the process of imputation, and the process of segmentation. In this paper, we propose a method to link both modality imputation and segmentation using convolutional neural networks. We show that by jointly optimizing the imputation network and the segmentation network, the method not only produces more realistic synthetic FLAIR images from T1-w images, but also improves the segmentation of WMH from T1-w images only. ",True,images/B1AxE-njf
SJhfv3ijz,Shuqing Chen-Xia Zhong-Shiyang Hu-Sabrina Dorn-Marc Kachelriess-Michael Lell-Andreas Maier,Automatic multi-organ segmentation in dual energy CT using 3D fully convolutional network,"Automatic multi-organ segmentation of the dual energy computed tomography
(DECT) data is beneficial for biomedical research and clinical applications. Numerous
recent researches in medical image processing show the feasibility to use 3-D
fully convolutional networks (FCN) for voxel-wise dense predictions of medical
images. In the scope of this work, three 3D-FCN-based algorithmic approaches
for the automatic multi-organ segmentation in DECT are developed. Both of the
theoretical benefit and the practical performance of these novel deep-learning-based
approaches are assessed. The approaches were evaluated using 26 torso DECT
data acquired with a clinical dual-source CT system. Six thoracic and abdominal
organs (left and right lungs, liver, spleen, and left and right kidneys) were evaluated
using a cross-validation strategy. In all the tests, we achieved the best average Dice
coefficients of 98% for the right lung, 97% for the left lung, 93% for the liver,
91% for the spleen, 94% for the right kidney, 92% for the left kidney, respectively.
Successful tests on special clinical cases reveal the high adaptability of our methods
in the practical application. The results show that our methods are feasible and
promising.",True,images/SJhfv3ijz
B1e_uuoif,Alexander Katzmann-Alexander Muehlberg-Michael Suehling-Dominik Noerenberg-Julian Walter Holch-Volker Heinemann-Horst-Michael Gross,Predicting Lesion Growth and Patient Survival in Colorectal Cancer Patients using Deep Neural Networks,"Being responsible for over 50,000 death per year within the U.S. alone, colorectal cancer (CRC) is the second leading cause of cancer related deaths in industry nations with increasing prevalence. Within the scope of personalized medicine, precise estimates on future progress are crucial. We thus propose a novel deep learning based system using deep convolutional sparse autoencoders for estimating future lesion growth for CRC liver lesions based on single slice CT tumor images for early therapy assessment. Furthermore, we show that our system can be used for one-year survival prediction in CRC patients. While state of the art treatment assessment (RECIST) is premised on retrospective lesion analysis, our proposed system delivers an estimate on future response, thus prospectively allowing to adapt therapy before further progress. We compare our system to single-lesion assessment through RECIST diameter and Radiomics. With our approach we archieve a phi-coefficient of 40.0% compared to 27.3% / 29.4% and an AUC of .784 vs .744/.737 for growth prediction, as well as a phi-coefficient of 44.9% vs 32.1% / 18.0% and an AUC of .710 vs. .688/.568 for survival prediction.",True,images/B1e_uuoif
BJtn7-3sM,Jo Schlemper-Ozan Oktay-Liang Chen-Jacqueline Matthew-Caroline Knight-Bernhard Kainz-Ben Glocker-Daniel Rueckert,Attention-Gated Networks for Improving Ultrasound Scan Plane Detection,"In this work, we apply an attention-gated network to real-time automated scan plane detection for fetal ultrasound screening. Scan plane detection in fetal ultrasound is a challenging problem due the poor image quality resulting in low interpretability for both clinicians and automated algorithms. To solve this, we propose incorporating self-gated soft-attention mechanisms. A soft-attention mechanism generates a gating signal that is end-to-end trainable, which allows the network to contextualise local information useful for prediction. The proposed attention mechanism is generic and it can be easily incorporated into any existing classification architectures, while only requiring a few additional parameters. We show that, when the base network has a high capacity, the incorporated attention mechanism can provide efficient object localisation while improving the overall performance. When the base network has a low capacity, the method greatly outperforms the baseline approach and significantly reduces false positives. Lastly, the generated attention maps allow us to understand the model's reasoning process, which can also be used for weakly supervised object localisation.",True,images/BJtn7-3sM
H1nGLZ2oG,Xiaoran Chen-Ender Konukoglu,Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders,"Lesion detection in brain Magnetic Resonance Images (MRI) remains a challenging task. State-of-the-art approaches are mostly based on supervised learning making use of large annotated datasets. Human beings, on the other hand, even non experts, can detect most abnormal lesions after seeing a handful of healthy brain images. Replicating this capability of using prior information on the appearance of healthy brain structure to detect lesions can help computers achieve human level abnormality detection, specifically reducing the need for number of labeled examples and better generalization to previously unseen lesions. To this end, we study detection of lesion regions in an unsupervised manner by learning data distribution of brain MRI of healthy subjects using auto-encoder based methods. We hypothesize that one of the main limitations of the current models is the lack of consistency in latent representation. We propose a simple yet effective constraint that helps mapping of an image bearing lesion close to its corresponding healthy image in the latent space. We use the Human Connectome Project dataset to learn distribution of healthy appearing brain MRI and report improved detection, in terms of AUC, of the lesions in the BRATS challenge dataset.",True,images/H1nGLZ2oG
HkmkmW2jM,Ameneh Sheikhjafari-Michelle Noga-Kumaradevan Punithakumar-Nilanjan Ray,Unsupervised deformable image registration with fully connected generative neural network,"In this paper, a new deformable image registration method based on a fully connected
neural network is proposed. Even though a deformation field related to the
point correspondence between fixed and moving images are high-dimensional in
nature, we assume that these deformation fields form a low dimensional manifold
in many real world applications. Thus, in our method, a neural network generates
an embedding of the deformation field from a low dimensional vector. This low-dimensional
manifold formulation avoids the intractability associated with the high
dimensional search space that most other methods face during image registration.
As a result, while most methods rely on explicit and handcrafted regularization of
the deformation fields, our algorithm relies on implicitly regularizing the network
parameters. The proposed method generates deformation fields from latent low
dimensional space by minimizing a dissimilarity metric between a fixed image
and a warped moving image. Our method removes the need for a large dataset
to optimize the proposed network. The proposed method is quantitatively evaluated
using images from the MICCAI ACDC challenge. The results demonstrate
that the proposed method improves performance in comparison with a moving
mesh registration algorithm, and also it correlates well with independent manual
segmentations by an expert.",True,images/HkmkmW2jM
r1s0gx3iG,Saskia M. Camps-Tim Houben-Davide Fontanarosa-Christopher Edwards-Maria Antico-Matteo Dunnhofer-Esther G.H.J. Martens-Jose A. Baeza-Ben G.L. Vanneste-Evert J. van Limbergen-Peter H.N. de With-Frank Verhaegen-Gustavo Carneiro,One-class Gaussian process regressor for quality assessment of transperineal ultrasound images ,"The use of ultrasound guidance in prostate cancer radiotherapy workflows is not widespread. This can be partially attributed to the need for image interpretation by a trained operator during ultrasound image acquisition. In this work, a one-class regressor, based on DenseNet and Gaussian processes, was implemented to assess automatically the quality of transperineal ultrasound images of the male pelvic region. The implemented deep learning approach achieved a scoring accuracy of 94%, a specificity of 95% and a sensitivity of 93% with respect to the majority vote of three experts, which was comparable with the results of these experts. This is the first step towards a fully automatic workflow, which could potentially remove the need for image interpretation and thereby make the use of ultrasound imaging, which allows real-time volumetric organ tracking in the RT environment, more appealing for hospitals. ",True,images/r1s0gx3iG
rJZz-knjz,Murat Seckin Ayhan-Philipp Berens,Test-time Data Augmentation for Estimation of Heteroscedastic Aleatoric Uncertainty in Deep Neural Networks,"Deep neural networks (DNNs) have revolutionized medical image analysis and disease diagnosis. Despite their impressive increase in performance, it is difficult to generate well-calibrated probabilistic outputs for such networks such that state-of-the-art networks fail to provide reliable uncertainty estimates regarding their decisions. We propose a simple but effective method using traditional data augmentation methods such as geometric and color transformations at test time. This allows to examine how much the network output varies in the vicinity of examples in the input spaces. Despite its simplicity, our method yields useful estimates for the input-dependent predictive uncertainties of deep neural networks. We showcase the impact of our method via the well-known collection of fundus images obtained from a previous Kaggle competition.",True,images/rJZz-knjz
S1jqMb2oM,Mostafa Mehdipour Ghazi-Mads Nielsen-Akshay Pai-M. Jorge Cardoso-Marc Modat-Sebastien Ourselin-Lauge Sørensen,Robust training of recurrent neural networks to handle missing data for disease progression modeling,"Disease progression modeling (DPM) using longitudinal data is a challenging task in machine learning for healthcare that can provide clinicians with better tools for diagnosis and monitoring of disease. Existing DPM algorithms neglect temporal dependencies among measurements and make parametric assumptions about biomarker trajectories. In addition, they do not model multiple biomarkers jointly and need to align subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. We, therefore, propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle missing values in both target and predictor variables. This algorithm is applied for modeling the progression of Alzheimer's disease (AD) using magnetic resonance imaging (MRI) biomarkers. The results show that the proposed LSTM algorithm achieves a lower mean absolute error for prediction of measurements across all considered MRI biomarkers compared to using standard LSTM networks with data imputation or using a regression-based DPM method. Moreover, applying linear discriminant analysis to the biomarkers' values predicted by the proposed algorithm results in a larger area under the receiver operating characteristic curve (AUC) for clinical diagnosis of AD compared to the same alternatives, and the AUC is comparable to state-of-the-art AUC's from a recent cross-sectional medical image classification challenge. This paper shows that built-in handling of missing values in LSTM network training paves the way for application of RNNs in disease progression modeling.",True,images/S1jqMb2oM
HJyajXC9G,Xinpeng Xie-Yuexiang Li-Linlin Shen,Active Learning for Breast Cancer Identification,"Breast cancer is the second most common malignancy among women and has become a major public health problem in current society. Traditional breast cancer identification requires experienced pathologists to carefully read the breast slice, which is laborious and suffers from inter-observer variations. Consequently, an automatic classification framework for breast cancer identification is worthwhile to develop. Recent years witnessed the development of deep learning technique. Increasing number of medical applications start to use deep learning to improve diagnosis accuracy. In this paper, we proposed a novel training strategy, namely reversed active learning (RAL), to train network to automatically classify breast cancer images. Our RAL is applied to the training set of a simple convolutional neural network (CNN) to remove mislabeled images. We evaluate the CNN trained with RAL on publicly available ICIAR 2018 Breast Cancer Dataset (IBCD). The experimental results show that our RAL increases the slice-based accuracy of CNN from 93.75% to 96.25%.",False,images/HJyajXC9G
HyqPGbhsG,Alain Jungo-Raphael Meier-Ekin Ermis-Evelyn Herrmann-Mauricio Reyes,Uncertainty-driven Sanity Check: Application to Postoperative Brain Tumor Cavity Segmentation,"Uncertainty estimates of modern neuronal networks provide additional information next to the computed predictions and are thus expected to improve the understanding of the underlying model. Reliable uncertainties are particularly interesting for safety-critical computer-assisted applications in medicine, e.g., neurosurgical interventions and radiotherapy planning. We propose an uncertainty-driven sanity check for the identification of segmentation results that need particular expert review. Our method uses a fully-convolutional neural network and computes uncertainty estimates by the principle of Monte Carlo dropout. We evaluate the performance of the proposed method on a clinical dataset with 30 postoperative brain tumor images. The method can segment the highly inhomogeneous resection cavities accurately (Dice coefficients 0.792 ± 0.154). Furthermore, the proposed sanity check is able to detect the worst segmentation and three out of the four outliers. The results highlight the potential of using the additional information from the model's parameter uncertainty to validate the segmentation performance of a deep learning model.",True,images/HyqPGbhsG
Skft7cijM,Ozan Oktay-Jo Schlemper-Loic Le Folgoc-Matthew Lee-Mattias Heinrich-Kazunari Misawa-Kensaku Mori-Steven McDonagh-Nils Y Hammerla-Bernhard Kainz-Ben Glocker-Daniel Rueckert,Attention U-Net: Learning Where to Look for the Pancreas,We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available. ,True,images/Skft7cijM
r1V_ovvjG,Lorenz Berger-Eoin Hyde-Nevil Pavithran-Faiz Mumtaz-Felix Bragman-M. Jorge Cardoso-Sebastien Ourselin,How to control the learning rate of adaptive sampling schemes,"Deep convolutional neural networks have shown excellent performance in object
recognition tasks and dense classification problems such as semantic segmentation. However training deep neural networks is still challenging and can require large amounts of computational resources to find network hyperparameters that result in good generalization properties. This procedure can be further complicated when an adaptive/boosted sampling scheme is used which varies the amount of information in mini-batches throughout training. In this work we address the task of tuning the learning rate schedule for Stochastic Gradient Descent (SGD) whilst employing an adaptive sampling procedure. We review recent theory of SGD training dynamics to help interpret our experimental findings, give a detailed description of the proposed algorithm for optimizing the SGD learning rate schedule and show that our method generalizes well and is able to attain state-of-art results on the VISCERAL Anatomy benchmark. ",False,images/r1V_ovvjG
HJ-PorsiG,Hao Xu-Jurgen E. Schneider-Alistair A. Young-Vicente Grau,Fully Automated Segmentation of the Left Ventricle in Small Animal Cardiac MRI,"The study of cardiovascular diseases requires viable animal models, and small animals have become models of choice due to their significant advantages. Segmentation from cine-MR image has become the gold standard for cardiac function assessment. While many image analysis methods have been developed for clinical studies, similar techniques are generally lacking for preclinical cases. Recent application of neural networks has shown encouraging results in several medical imaging applications. For cardiac cine-MR image segmentation, convolutional neural networks and recurrent neural networks have been successfully used for segmentation of the left ventricle. However, most methods only use a stack of short axis images, which introduces inaccuracy and uncertainty for cardiac function assessment in 3D, especially considering the existence of misalignment between slices. In this paper, we propose an efficient and 3D consistent segmentation method for small animal cardiac MR images taking advantage of a combination of long-axis and short-axis images, by combining convolutional neural networks and the guide-point modelling method. Unlike in most clinical studies, we also focus on training with small datasets, as is common in preclinical studies, and show accurate results with only 12 cine-MR sequences.",False,images/HJ-PorsiG
ry8Fki9of,Aydan Gasimova-Liang Chen-Paul Bentley-Giovanni Montana-Daniel Rueckert,Learning Attention from Multi-Modal Imaging and Text: Application for Lesion Localisation in DWI,"Strokes are one of the leading causes of death and disability in the UK. There are two main types of stroke: ischemic and hemorrhagic, with the majority of stroke patients suffering from the former. During an ischemic stroke, parts of the brain lose blood supply, and if not treated immediately, can lead to irreversible tissue damage and even death. Ischemic lesions can be detected by diffusion weighted magnetic resonance imaging (DWI), but localising and quantifying these lesions can be a time consuming task for clinicians. Work has already been done in training neural networks to segment these lesions, but these frameworks require a large amount of manually segmented 3D images, which are very time consuming to create. We instead propose to use past examinations of stroke patients which consist of DWIs, corresponding radiological reports and diagnoses in order to develop a learning framework capable of localising lesions. This is motivated by the fact that the reports summarise the presence, type and location of the ischemic lesion for each patient, and thereby provide more context than a single diagnostic label. Localisation of lesions is aided by an attention mechanism which implicitly learns which regions within the DWI are most relevant to the classification.",False,images/ry8Fki9of
r1ZGQW2if,Matthew D. Sinclair-Juan Cerrolaza Martinez-Emily Skelton-Yuanwei Li-Christian F. Baumgartner-Wenjia Bai-Jacqueline Matthew-Caroline L. Knight-Sandra Smith-Jo Hajnal-Andrew P. King-Bernhard Kainz-Daniel Rueckert,Cascaded Transforming Multi-task Networks For Abdominal Biometric Estimation from Ultrasound,"Measurement of biometrics from fetal ultrasound (US) images is of key importance in monitoring healthy fetal development. Under the time-constraints of a clinical setting however, accurate measurement of relevant anatomical structures, including abdominal circumference (AC), is subject to large inter-observer variability. To address this, an automated method is proposed to annotate the abdomen in 2D US images and measure AC using a shape-aware, multi-task deep convolutional neural network in a cascaded model framework. The multi-task loss simultaneously optimises both pixel-wise segmentation and shape parameter regression. We also introduce a cascaded shape-based transformation to normalise for position and orientation of the anatomy, improving results further on challenging images. Models were trained using approximately 1700 abdominal images and compared to inter-expert variability on 100 test images. The proposed model performs better than inter-expert variability in terms of mean absolute error for AC measurements (2.60mm vs 5.89mm), and Dice score (0.962 vs 0.955). We also show that on the most challenging test images, the proposed method significantly improves on the baseline model, while running at 8fps which could aid clinical workflow.",True,images/r1ZGQW2if
B1n4ZKsjG,Karine Seymour-Pierre Payoux,"Radiomics Enabler®, an ETL (Extract-Transform-Load) for biomedical imaging in big-data projects","Clinical data warehouses are now routinely used by large hospitals. They allow researchers to pool medical data from millions of patients to create unified and harmonized patient cohorts for clinical trials and epidemiology studies. In bio-medical imaging, although there is a need to extract large amounts of data to help develop decision tools based on artificial intelligence, the identification, ex-traction and anonymization of relevant sequences is traditionally very time-consuming, as it is done patient by patient. We developed Radiomics Enabler® to streamline this process. We compared the extraction and anonymization pro-cess with and without Radiomics Enabler® on 118 imaging exams. We found that the operator’s time required using his PACS (Picture Archiving and Commu-nication System) workstation was 5 hours and 37 minutes. The time was reduced to only 17 minutes with Radiomics Enabler®. Radiomics Enabler® is available as open-source and can be integrated with any clinical data warehouse.",False,images/B1n4ZKsjG
BkByNuojz,Kamal Souadih-Ahror Belaid-Douraied Ben Salem,Fully Automatic Segmentation of Sphenoid Sinus in CT Images with 3D Convolutional Neural Networks,"Today, Deep learning algorithms have quickly become essential in the field of medical image analysis. Compared to the traditional methods, these Deep learning techniques are more efficient in extracting compact information leading towards significant improvement performance of medical image analysis system. We present in this paper a new technique for Sphenoid sinus automatic segmentation using a 3D Convolutional Neural Networks (CNN). Due to the scarcity of medical data, we chose to used a 3D CNN model learned on a small training set. Mathematical morphology operations are then used to automatically detect and segment the region of interest. Our proposed method is tested and compared with a semi-automatic method and manual delineations made by a specialist. The preliminary results from the  Head Computed Tomography (CT) volumes
seem to be very promising.",False,images/BkByNuojz
Sk-MRujoz,David Alonso-Caneiro-Scott A. Read-Jared Hamwood-Stephen J. Vincent-Michael J. Collins,Use of convolutional neural networks for the automatic segmentation of total retinal and choroidal thickness in OCT images,"The assessment of total retinal and choroidal thickness from optical coherence tomography (OCT) images is an important clinical and research task. These thickness measures and their changes represent a fundamental metric extracted from OCT data, since they provide valuable information regarding the eye’s normal anatomy and physiology. Changes in thickness are associated with natural eye development, the progression of various eye diseases, and the development of refractive error. Manual analysis of OCT images is time-consuming and not feasible for large datasets of images. Thus, the development of reliable and accurate methods to automatically segment tissue boundaries in OCT images is fundamental. In this paper, convolutional neural networks (CNNs) are used to calculate the probability of boundary locations in OCT images. The CNN, trained using image patches centred around the boundary of interest, provides a per-layer probability map that marks the most likely predicted location of the boundaries. This map is subsequently traced using a graph-search approach to segment the boundaries. The effect of patch size, network architecture and input image pre-processing on the CNN performance and subsequent layer segmentation is presented. The results are compared with manual image segmentation as well as a fully convolutional network. This work may support the future development of CNN methods for automated OCT boundary segmentation.",False,images/Sk-MRujoz
r1Q98pjiG,Pranav Rajpurkar-Jeremy Irvin-Aarti Bagul-Daisy Ding-Tony Duan-Hershel Mehta-Brandon Yang-Kaylie Zhu-Dillon Laird-Robyn L. Ball-Curtis Langlotz-Katie Shpanskaya-Matthew P. Lungren-Andrew Y. Ng,MURA Dataset: Towards Radiologist-Level Abnormality Detection in Musculoskeletal Radiographs,"We introduce MURA, a large dataset of musculoskeletal radiographs containing 40,562 images from 14,864 studies, where each study is manually labeled by radiologists as either normal or abnormal. On this dataset, we train a 169-layer densely connected convolutional network to detect and localize abnormalities. To evaluate our model robustly and to get an estimate of radiologist performance, we collect additional labels from six board-certified Stanford radiologists on the test set, consisting of 207 musculoskeletal studies. On this test set, the majority vote of a group of three radiologists serves as gold standard. The model achieves an AUROC of 0.929, with an operating point of 0.815 sensitivity and 0.887 specificity. We also compare our model and radiologists on the Cohen's kappa statistic, which expresses the agreement of our model and of each radiologist with the gold standard. We find that our model achieves performance comparable to that of radiologists. Model performance is comparable to the best radiologist performance in detecting abnormalities on finger and wrist studies. However, model performance is lower than best radiologist performance in detecting abnormalities on elbow, forearm, hand, humerus, and shoulder studies, indicating that the task is a good challenge for future research. To encourage advances, we have made our dataset freely available at http://stanfordmlgroup.github.io/competitions/mura.",True,images/r1Q98pjiG
Bk_faUosM,Shuanlong Che-Chao Li-Pifu Luo-Longsen Chen,Morphological Detection Of Helicobector Pyloric Organisms On Gastric Mucosa Using Deep Learning Of The Artificial Intelligence,"Since discovered, Helicobacter pylori (H. pylori) is acknoledged as one of the major causes of gastric ulcer, duodenal ulcer, gastritis, and gastric cancer. Detecting the infection of H. pylori in human body is of great significance for the treatment of various gastrointestinal diseases caused by Helicobacter pylori. This paper provides a new method  of dectecting  H. pylori based on digital pathology and deep learning. Through the study of a large number of whole slide images (WSIs), the detection of H. pylori on WSIs from gastric biopsy is achieved. The experimental results in this paper show that this method can achieve good detection performance and has certain promotion and practical value.",False,images/Bk_faUosM
BktMD6isM,Daniele Ravì-Agnieszka Barbara Szczotka-Dzhoshkun Ismail Shakir-Stephen P Pereira-Tom Vercauteren,Adversarial training with cycle consistency for unsupervised super-resolution in endomicroscopy,"In recent years, endomicroscopy imaging has become increasingly used for diagnostic purposes. It can provide intraoperative aids for real-time tissue characterization and can help to perform visual investigations aimed to discover epithelial cancers. However, accurate diagnosis and correct treatments are partially hampered by the low numbers of informative pixels generated by these devices. In the last decades, progress has been made to improve the hardware acquisition and the related image reconstruction in this domain. Nonetheless, due to the imaging environment, and the associated physical constraints, images with the desired resolution are still difficult to produce. Post-processing techniques, such as Super Resolution (SR), are an alternative solution to increase the quality of these images. SR techniques are often supervised, requiring aligned pairs of low-resolution (LR) and high-resolution (HR) patches to train a model. However, in some domains, the lack of HR images hinders the generation of these pairs and makes supervised training unsuitable. For this reason, we propose an unsupervised SR framework based on an adversarial deep neural network with a physically-inspired cycle consistency, designed to impose some acquisition properties on the super-resolved images. Our framework can exploit HR images, regardless of the domain where they are coming from, to transfer the super-resolution to the initial LR images. This property can be particularly useful in all situations where pairs of LR/HR are not available during the training. Our quantitative analysis, validated using a database of 238 endomicroscopy video sequences, shows the ability of the pipeline to produce convincing super-resolved images. A Mean Opinion Score (MOS) study also confirms this quantitative image quality assessment.   
",True,images/BktMD6isM
SygUuqjjf,Xiong Yan-Hou Ao-Li Ting-Chen Longsen-Chen Lifang-Lai Lili,Multilayer Scanning Enhances Sensitivity Of Artificial Intelligence-aided Mycobacterium Tuberculosis Detection,"In the study of automatic detection of Mycobacterium Tuberculosis (TB) using artificial intelligence, 201 samples (108 positive cases and 93 negative cases) were collected as a test set and used to examine TB-AI and TB-AI achieved 97.94% sensitivity and 83.65% specificity. However, with single-layer scanning, some Mycobacterium TBs are blurred due to the defocus. As a result, slides with blurred TB pixels may not be detected as positive In this paper a new test of TB-AI with three-layer scanning was conducted on 189 positive cases reported by medical doctors with microscope. Comparing to the ordinary single-layer scanned slides, additional 6 out of 189 cases (3.2%) were detected.",False,images/SygUuqjjf
HJZ2BKisz,Jie Wang-Chong Chen-Fei Li-Zhe Wang-Guoxiang Qu-Yu Qiao-Hairong Lv-Xiulan Zhang,S-D Net: Joint Segmentation and Diagnosis Revealing the Diagnostic Significance of Using Entire RNFL Thickness in Glaucoma,"Glaucoma is a severe eye disease causing blindness. The early diagnosis of glaucoma is of great importance and mainly based on the detection of early signs of optic neuropathy, including retinal nerve fiber layer (RNFL) thinning. We design S-D net to implement automatic segmentation of retinal layers in optical coherence tomography (OCT) images and diagnosis of glaucoma using RNFL thickness vector calculated from the segmentation maps. S-D net is an end-to-end optimized system that mimics the behavior of an ophthalmologist for diagnosing glaucoma with the OCT report. The specially designed unit layer in S-D net gives the threshold of thickness at each point of the RNFL in glaucoma diagnosis. Our results show that S-D net distinguishes glaucoma from healthy cases according to the distribution and magnitude of RNFL thickness. Our model achieves state-of- the-art segmentation results and competitive diagnosing accuracy compared with an experienced ophthalmologist.",False,images/HJZ2BKisz
S1aY66iiM,Yi Li-Wei Ping,Cancer Metastasis Detection With Neural Conditional Random Field,"Breast cancer diagnosis often requires accurate detection of metastasis in lymph nodes through Whole-slide Images (WSIs). Recent advances in deep convolutional neural networks (CNNs) have shown significant successes in medical image analysis and particularly in computational histopathology. Because of the outrageous large size of WSIs, most of the methods divide one slide into lots of small image patches and perform classification on each patch independently. However, neighboring patches often share spatial correlations, and ignoring these spatial correlations may result in inconsistent predictions. In this paper, we propose a neural conditional random field (NCRF) deep learning framework to detect cancer metastasis in WSIs. NCRF considers the spatial correlations between neighboring patches through a fully connected CRF which is directly incorporated on top of a CNN feature extractor. The whole deep network can be trained end-to-end with standard back-propagation algorithm with minor computational overhead from the CRF component. The CNN feature extractor can also benefit from considering spatial correlations via the CRF component. Compared to the baseline method without considering spatial correlations, we show that the proposed NCRF framework obtains probability maps of patch predictions with better visual quality. We also demonstrate that our method outperforms the baseline in cancer metastasis detection on the Camelyon16 dataset and achieves an average FROC score of 0.8096 on the test set. NCRF is open sourced at https://github.com/baidu-research/NCRF.
",True,images/S1aY66iiM
HkmVNFijf,Pádraig Looney-Gordon N. Stevenson-Kypros H. Nicolaides-Walter Plasencia-Malid Molloholli-Stavros Natsis-Sally Collins,Optimisation of a convolutional neural network to segment the first trimester placenta from 3D ultrasound scans,"Screening for increased risk of pregnancy complications could be possible with fully automated placental segmentation in 3D ultrasound (3D-US). Fully convolutional neural networks (fCNN) have previously obtained good segmentation performance of the first trimester placenta and appears to predict fetal growth restriction better than manual segmentation methods. The goal of this study is to adjust fCNN architecture parameters to investigate their impact on performance and ultimately to produce a more accurate segmentation. 2,393 first trimester 3D-US volumes with ‘ground-truth’ segmentation obtained using a semi-automated technique were used. An open source package (OxNNet) was used to train end-to-end six fully convolutional neural networks with different loss functions, addition of batch normalisation and with different numbers of features. A small increase in performance of placental segmentation in terms of Dice similarity coefficient (DSC) (0.835 vs 0.825) was observed. Doubling the feature map gave a minor improvement in DSC (0.01). Use of batch normalisation increased the speed of training as expected. The Dice-based loss gave poorer performance in general. Convolution with no padding produced better segmentation than using padding. The subjective case quality assessment score was shown to correlate with the DSC (r = -0.28 (p < 0.05)). A faster, less-memory intensive fCNN architecture can provide a similar segmentation performance moving the use of this tool for clinical screening a step closer.",False,images/HkmVNFijf
rkZOp9joz,Junni Shou-Yan Li-Guanzhen Yu-Guannan Li,Whole Slide Image Classification of Gastric Cancer using Convolutional Neural Networks,"Gastric cancer is one of the main causes of cancer and cancer-related mortality worldwide, and the diagnosis based on histopathology images is a gold standard for gastric cancer detection. However, manual diagnosis is labor-intensive and low in inter-observer agreement. Computer-aided image analysis method were thus developed to alleviate the workload of pathologists and overcome the problem of subjectivity. Histopathology image analysis using deep learning has been proved to give more promising results than traditional methods on many whole slide image cancer detection tasks, including breast cancer detection and prostate cancer detection. In this paper, we further studied a whole slide image classification method using Convolutional Neural Networks (CNNs) on gastric cancer data. The method classify a whole slide image based on patch-sized classification results. Various experiments for patch-level classification using different existing CNN architectures were conducted. Experiment results show that the architecture gives the state-of-the-art result in natural image classification tasks can also give impressive results in histopathology image classification tasks.",False,images/rkZOp9joz
rJD6Xgnoz,Kai Lønning-Patrick Putzky-Matthan W. A. Caan-Max Welling,Recurrent Inference Machines for Accelerated MRI Reconstruction,"Accelerated MRI reconstruction is important for making MRI faster and thus applicable in a broader range of problem domains.  Computational tools allow for high-resolution imaging without the need to perform time-consuming measurements. Most recently, deep learning approaches have been applied to this problem. However, none of these methods have been shown to transfer well across different measurement settings. We propose to use Recurrent Inference Machines as a framework for accelerated MRI, which allows us to leverage the power of deep learning without explicit domain knowledge. We show in experiments that the model can generalize well across different setups, while at the same time it outperforms another deep learning method and a compressed sensing approach.",True,images/rJD6Xgnoz
BkF7Ypijf,Harinarayanan K K-Nirmal Jith OU,Deep neural network based assistive screening for cervical cancer,"Automating cervical cancer screening has the potential to reduce high mortality
rates due to cervical cancer, especially in developing countries. The most promising
of these techniques is assisted screening in which preliminary analysis is validated
by a pathologist. Assisted screening requires classification algorithms for initial
screening that is later validated by a pathologist. It also needs attention, detection
or segmentation algorithms for drawing attention of pathologists to important
regions. Existing algorithms for cervical cancer screening focus on classification
of individual cells . This focus leads to need for accurate segmentation of cells and
inability to use extracellular information. In this work we propose a segmentation
free deep learning algorithm for classification of PAP smear images. The proposed
algorithm uses the intrinsic information in the network to generate a map of
important regions for the pathologist to look into. This map is generated with sub
image resolution while the training data contains annotations at the image level
only. Our analysis on a dataset of around 14000 images validates the approach of
assisted screening in reducing the pathologist workload by a large factor.",False,images/BkF7Ypijf
BkuKMztoG,Jocelyn Desbiens-Sunil Gupta-Jonathan Stevenson-Allan Alderman-Anusua Trivedi-Patrick Buehler,"Deep Annotated Learning, Harmonic Descriptors and Automated Diabetic Retinopathy Detection","Identifying candidate regions in medical images is of greatest importance since it provides intuitive illustrations for doctors and patients of how the diagnosis is inferred. Recently, advances in Deep Learning have dramatically improved the performance of automated Diabetic Retinopathy (DR) detection. Most of these Deep Learning systems treat Convolutional Neural Network (CNN) as a kind of black box, lacking comprehensive explanation. Our proposed system learns from image-level pre-processed annotations of DR highlighting suspicious regions through harmonic vasculature recosntruction. It mimics the expert process of a clinician examining an image by selecting regions showing high probability of being lesions. Then annotated images are passed to a CNN which in turn predicts their respective DR severity. 
Using annotated images for training/testing has also the clear advantage of increasing detection accuracy. On a clinical data sets of fully gradable images, the algorithm achieved an accuracy of 97.1% with a sensitivity of 96.6% and a specificity of 98.0% for an AUC value of 99.5%. On the publicly available Messidor-2 image dataset, sensitivity of 92.9% and specificity of 98.9% were achieved.
For No DR vs. Non-Sight Threatening DR the accuracy was 89.5% with sensitivity of 87.5% and specificity of 91.8%. No DR vs. Sight Threatening DR achieved an accuracy of 97.9% with sensitivity of 97.9% and specificity of 98.4%. Meanwhile, for Non-Sight Threatening DR vs. Sight Threatening DR the accuracy was 79.7% with sensitivity of 77.7% and specificity of 81.7%.
",False,images/BkuKMztoG
Byxv9aioz,Guotai Wang-Wenqi Li-Michael Aertsen-Jan Deprest-Sebastien Ourselin-Tom Vercauteren,Test-time augmentation with uncertainty estimation for deep learning-based medical image segmentation,"Data augmentation has been widely used for training deep learning systems for medical image segmentation and plays an important role in obtaining robust and transformation-invariant predictions. However, it has seldom been used at test time for segmentation and not been formulated in a consistent mathematical framework. In this paper, we first propose a theoretical formulation of test-time augmentation for deep learning in image recognition, where the prediction is obtained through estimating its expectation by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We then propose a novel uncertainty estimation method based on the formulated test-time augmentation. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions, and 2) it provides a better uncertainty estimation than calculating the model-based uncertainty alone and helps to reduce overconfident incorrect predictions.",False,images/Byxv9aioz
SJ4N7isiG,Jelmer M. Wolterink-Tim Leiner-Ivana Isgum,Blood Vessel Geometry Synthesis using Generative Adversarial Networks,"Computationally synthesized blood vessels can be used for training and evaluationof medical image analysis applications. We propose a deep generative model to synthesize blood vessel geometries, with an application to coronary arteries in cardiac CT angiography (CCTA).

In the proposed method, a Wasserstein generative adversarial network (GAN) consisting of a generator and a discriminator network is trained. While the generator tries to synthesize realistic blood vessel geometries, the discriminator tries to distinguish synthesized geometries from those of real blood vessels. Both real and synthesized blood vessel geometries are parametrized as 1D signals based on the central vessel axis. The generator can optionally be provided with an attribute vector to synthesize vessels with particular characteristics.

The GAN was optimized using a reference database with parametrizations of 4,412 real coronary artery geometries extracted from CCTA scans. After training, plausible coronary artery geometries could be synthesized based on random vectors sampled from a latent space. A qualitative analysis showed strong similarities between real and synthesized coronary arteries. A detailed analysis of the latent space showed that the diversity present in coronary artery anatomy was accurately captured by the generator.

Results show that Wasserstein generative adversarial networks can be used to synthesize blood vessel geometries.",True,images/SJ4N7isiG
HyUmbjsiz,Eva Costa-Nelson Martins-Malik Saad Sultan-Diana Veiga-Manuel Ferreira-Sandra Mattos-Miguel Coimbra,Mitral Valve Leaflets Segmentation in Echocardiography using Convolutional Neural Networks,"Rheumatic heart disease remains a major burden in the developing countries. The World Heart Federation proposed guidelines for the echocardiographic detection of the disease, in which the mitral leaflets’ morphology assessment is a key indicator. The drawback is that these guidelines are dependent on the clinician experience. To overcome this limitation, we propose an automatic segmentation of the mitral leaflets using a new method based on convolutional neural network, specifically the UNet architecture. The results indicate a median DICE coefficient of 0.74 in PLAX and 0.79 in A4C for the anterior mitral leaflet segmentation, while median DICE of 0.60 in PLAX and 0.69 A4C are met for the posterior leaflet. A visual evaluation of this segmentation approach by two cardiologists is in line with the numerical results. The false detection due to overestimation and artifacts remains an issue to be addressed in the future.",False,images/HyUmbjsiz
SkciMAjiG,Pedro Costa-Teresa Araújo-Guilherme Aresta-Adrian Galdran-Ana Maria Mendonça-Asim Smailagic-Aurélio Campilho,EyeWeS: Weakly Supervised Pre-Trained Convolutional Neural Networks for Diabetic Retinopathy Detection,"Diabetic Retinopathy (DR) is one of the leading causes of preventable blindness in the developed world. With the increasing number of diabetic patients there is a growing need of an automated system for DR detection. We propose EyeWeS, a general methodology that enables the conversion of any pre-trained convolutional neural network into a weakly-supervised model while at the same time achieving an increased performance and efficiency. Via EyeWeS, we are able to design a new family of methods that can not only automatically detect DR in eye fundus images, but also pinpoint the regions of the image that contain lesions, while being trained exclusively with image labels. EyeWeS improved the results of Inception V3 from 94.9% Area Under the Receiver Operating Curve (AUC) to 95.8% AUC while maintaining only approximately 5% of the Inception V3’s number of parameters. The same model is able to achieve 97.1% AUC in a cross-dataset experiment. In the same cross-dataset experiment we also show that EyeWeS Inception V3 is effectively detecting microaneurysms and small hemorrhages as the indication of DR.",False,images/SkciMAjiG
HJq5OGKsz,Michael Ferlaino-Craig A. Glastonbury-Carolina Motta-Mejia-Manu Vatish-Ingrid Granne-Stephen Kennedy-Cecilia M. Lindgren-Christoffer Nellaker,Towards Deep Cellular Phenotyping in Placental Histology,"The placenta is a complex organ, playing multiple roles during fetal development. Very little is known about the association between placental morphological abnormalities and fetal physiology. In this work, we present an open sourced, computationally tractable deep learning pipeline to analyse placenta histology at the level of the cell. By utilising two deep Convolutional Neural Network architectures and transfer learning, we can robustly localise and classify placental cells within five classes with an accuracy of 89%. Furthermore, we learn deep embeddings encoding phenotypic knowledge that is capable of both stratifying five distinct cell populations and learn intraclass phenotypic variance. We envisage that the automation of this pipeline to population scale studies of placenta histology has the potential to improve our understanding of basic cellular placental biology and its variations, particularly its role in predicting adverse birth outcomes.",True,images/HJq5OGKsz
SJ_-Nx3jz,Felix Ambellan-Alexander Tack-Moritz Ehlke-Stefan Zachow,Automated Segmentation of Knee Bone and Cartilage combining Statistical Shape Knowledge and Convolutional Neural Networks: Data from the Osteoarthritis Initiative,"We present a method for the automated segmentation of knee bones and cartilage from magnetic resonance imaging, that combines a priori knowledge of anatomical shape with Convolutional Neural Networks (CNNs). The proposed approach incorporates 3D Statistical Shape Models (SSMs) as well as 2D and 3D CNNs to achieve a robust and accurate segmentation of even highly pathological knee structures. The method is evaluated on data of the MICCAI grand challenge ""Segmentation of Knee Images 2010"".  For the first time an accuracy equivalent to the inter-observer variability of human readers has been achieved in this challenge. Moreover, the quality of the proposed method is thoroughly assessed using various measures for 507 manual segmentations of bone and cartilage, and 88 additional manual segmentations of cartilage.  Our method yields sub-voxel accuracy. In conclusion, combining of anatomical knowledge using SSMs with localized classification via CNNs results in a state-of-the-art segmentation method.",True,images/SJ_-Nx3jz
rJlGvTojG,Nidhi Ranjan-Pranav Vinod Machingal-Sunil Sri Datta Jammalmadka-Veena Thenaknidiyoor-and A. D. Dileep,Hierarchical Approach for Breast Cancer Histopathology Images Classification,"Cancer is one of the highest death causing diseases in the world, with breast cancer being the highest in women. Early stage detection of breast cancer can make the treatment process more effective. Typically the identification of stages of a cancer is performed by pathologists by analysing histopathology images/slides.Analysing  histopathological  images  is  a  highly  tedious  task  that  requires  long attention span. In this paper we propose an approach to classify histopathology images. In this work we lead to a CNN-based Hierarchical classifier to classify histopathology images into various stages of cancer. Deep learning in the field of Medical Image Analysis has shown propitious results.The effectiveness of the proposed CNN-based Hierarchical classifier is studied using the BACH challenge dataset. The proposed method achieved an accuracy of 95% on our validation set.",False,images/rJlGvTojG
HJJKiooof,Patrick Sousa-Adrian Galdran-Pedro Costa-Aurélio Campilho,Learning to Segment the Lung Volume from CT scans based on Semi-Automatic Ground-Truth,"Lung volume segmentation is a relevant task within the design of Computer-Aided Diagnosis systems related to automated lung pathology analysis. Isolating the lung from CT volumes can be a challenging process due to the considerable deformations and pathologies that can appear in different scans. Deep neural networks can be an effective mechanism in order to model the spatial relationship between different lung voxels. Unfortunately, this kind of models typically require large quantities of annotated data, and manually delineating the lung from volumetric CT scans can be a cumbersome process. In this paper, we propose to train a 3D Convolutional Neural Network to solve this task based on semi-automatically generated annotations. To achieve this goal, we introduce an extension of the well-known V-Net architecture that can handle higher-dimensional input data. Even if the training set labels are noisy and may contain some errors, we experimentally show that it is possible to learn to accurately segment the lung relying on them. Numerical comparisons performed on an external test set containing lung segmentations provided by a medical expert demonstrate that the proposed model generalizes well to new data, reaching an average 98.7% Dice coefficient. In addition, the proposed approach results in a superior performance when compared to the standard V-Net model, particularly on the lung boundary, achieving a 0.576 mm Average Symmetric Surface Distance with respect to expert validated ground-truth.",False,images/HJJKiooof
rkaSvlnoG,Youngjoo Seo-Manuel Morante-Yannis Kopsinis-Sergios Theodoridis,Unsupervised learning of the brain connectivity dynamic using residual D-net,"In this paper, we propose a novel unsupervised learning method to learn the brain dynamics using a deep learning architecture named residual D-net. As it is often the case in medical research, in contrast to typical deep learning tasks, the size of the resting-state functional Magnetic Resonance Image (rs-fMRI) datasets for training is limited. Thus, the available data should be very efficiently used to learn the complex patterns underneath the brain connectivity dynamics. To address this issue, we use residual connections to alleviate the training complexity through recurrent multi-scale representation. We conduct two classification tasks to differentiate early and late stage Mild Cognitive Impairment (MCI) from Normal healthy Control (NC) subjects. The experiments verify that our proposed residual D-net indeed learns the brain connectivity dynamics, leading to significantly higher classification accuracy compared to previously published techniques. ",False,images/rkaSvlnoG
r14Qkwsif,Xu Meiquan-Zeng Weixiu-Sun Yanhua-Wu Junhui-Wu Tingting-Yang Yajie-Zhang Meng-Zhu Zeji-Chen Longsen,Cervical Cytology Intelligent Diagnosis Based On Object Detection Technology,"Objectives: (i)Explore a new method for applying artificial intelligence to cervical cytology diagnosis. (ii)Realize an automatic detection system of positive cervical squamous epithelial cell. 
Methods: (i) The method can be divided into two phases: training and testing. (ii) For the training phases, first of all, we collect 500 cervical cell slides. Through scanning, annotating, and extracting patches, we get the training set, validation set and test set. Then, based on the Faster R-CNN (Faster Regions with CNN features) , we construct a neural network model for cell detection and classification. After continuous training, validation, analysis and tuning, we finally get a well trained neural network model. (iii) During the testing phase, we use the previously well trained neural model to predict the test set which consists of 100 whole slide images of cervical cytology. The model detects the five types of target cells at first, and then counts the number of cells in each category, finally, generates a diagnosis.
Results: The positive precision rate on the validation set is 0.91. On the test set, for two-class problem,the accuracy is 0.78. For four-class problem, the accuracy is 0.70.
Conclusion: Object detection technology has unique advantages in applying to cervical cytology. Through accurate detection and classification of various types of abnormal cells, as well as the statistics of each category, a comprehensive conclusion is made. This idea adopted in this study accords with doctors' traditional diagnosis process to a certain degree. Results show that the intelligent system realized with deep learning technology has the advantages of high speed, high consistency, and well diagnostic performance.",False,images/r14Qkwsif
H1hlwg2oG,Edward J. Holupka-Ph.D.-John Rossman-M.S.-Tye Morancy-M.S.-Joseph Aronovitz-M.D.-Ph.D.-Irving D. Kaplan-M.D.,The Detection of Implanted Radioactive Seeds On Ultrasound Images Using Convolution Neural Networks ,"Purpose: A common modality for the treatment of early stage prostate cancer is the implantation of radioactive seeds directly into the prostate. The radioactive seeds are positioned inside the prostate to achieve optimal radiation dose coverage to the prostate. These radioactive seeds are positioned inside the prostate using Transrectal ultrasound imaging. Once all of the planned seeds have been implanted, two dimensional transaxial transrectal ultrasound images separated by 2 mm are obtained through out the prostate, beginning at the base of the prostate up to and including the apex. 
Methods: A common deep neural network, called DetectNet was trained to automatically determine the position of the implanted radioactive seeds within the prostate under ultrasound imaging. 
Results: The results of the training using 950 training ultrasound images and 90 validation ultrasound images. The commonly used metrics for successful training were used to evaluate the efficacy and accuracy of the trained deep neural network and resulted in an loss_bbox (train) = 0.00, loss_coverage (train) = 1.89e-8, loss_bbox (validation) = 11.84, loss_coverage (validation) = 9.70, mAP (validation) = 66.87%, precision (validation) = 81.07%, and a recall (validation) = 82.29%, where train and validation refers to the training image set and validation refers to the validation training set. On the hardware platform used, the training expended 12.8 seconds per epoch. The network was trained for over 10,000 epochs. In addition the seed locations as determined by the Deep Neural Network were compared to the seed locations as determined by a commercial software based on a one to three months after implant CT. The Deep Learning approach was within 2.29 mm of the seed locations determined by the commercial software. 
Conclusions: The Deep Learning approach to the determination of radioactive seed locations is robust, accurate, and fast and well within spatial agreement with the gold standard of CT determined seed coordinates. 
",False,images/H1hlwg2oG
SJBA1khjz,Konstantin Thierbach-Pierre-Louis Bazin-Walter de Back-Filippos Gavriilidis-Evgeniya Kirilina-Carsten Jaeger-Markus Morawski-Stefan Geyer-Nikolaus Weiskopf-Nico Scherf,"Combining Deep Learning and Active Contours Opens The Way to Robust, Automated Analysis of 3D Brain Cytoarchitectonics","Deep learning has thoroughly changed the field of image analysis yielding impressive results whenever enough annotated data can be gathered. While partial annotation can be very fast, manual segmentation of 3D biological structures is tedious and error-prone. Additionally, high-level shape concepts such as topology or boundary smoothness are hard if not impossible to encode in Feedforward Neural Networks. Here we present a modular strategy for the accurate segmentation of neural cell bodies from light-sheet microscopy combining mixed-scale convolutional neural networks and topology-preserving geometric deformable models. We show that the network can be trained efficiently from simple cell centroid annotations, and that the final segmentation provides accurate cell detection and smooth segmentations that do not introduce further cell splitting or merging. The cell detection stage works sufficiently robust to even uncover actual errors in the reference annotations.",False,images/SJBA1khjz
S1Ec-bnof,Sejin Park-Woochan Hwang-Kyu-Hwan Jung,Semi-Supervised Reinforced Active Learning for Pulmonary Nodule Detection in Chest X-rays,"Machine learning applications in medical imaging are frequently limited by the lack of quality labeled data. While conventional active learning approaches have been able to reduce the labeling burden to some extent, the main difficulty was defining an effective sampling criteria. In this work, we propose a novel framework, semi-supervised reinforced active learning, which utilizes inverse reinforcement learning and an actor critic network to train a reward based active learning algorithm. This is an extension of the reinforced active learning formulation to complex problems where direct rewards may be unavailable. The framework was tested on a U-Net segmentation network for pulmonary nodules in chest X-rays. The proposed framework was able to achieve the same level of performance as the standard U-Net while using only 50% of the labeled data, demonstrating ability to effectively reduce the labeling burden.",False,images/S1Ec-bnof
ByL_UAjsG,Rüdiger Göbl-Julia Rackerseder-Nassir Navab-Christoph Hennersperger,Fully Automatic Segmentation of 3D Brain Ultrasound: Learning from Coarse Annotations,"Intra-operative ultrasound is an increasingly important imaging modality in neuro- surgery. However, manual interaction with imaging data during the procedures, for example to select landmarks or perform segmentation, is difficult and can be time consuming.  Yet, as registration to other imaging modalities is required in most cases, some annotation is necessary. We propose a segmentation method based on DeepVNet and specifically evaluate the integration of pre-training with simulated ultrasound sweeps to improve automatic segmentation and enable a fully automatic initialization of registration.  Trained on coarse and incomplete semi-automatic annotations, our approach is able to capture the desired superficial structures such as sulci , the cerebellar tentorium , and the falx cerebri. We perform a five-fold cross-validation on the publicly available RESECT dataset. Trained on the dataset alone, we report a Dice and Jaccard coefficient of 0.45 ± 0.09 and 0.30 ± 0.07 respectively, as well as an average distance of 0.78 ± 0.36 mm.  With the suggested pre-training, we computed a Dice and Jaccard coefficient of 0.47 ± 0.10 and 0.31 ± 0.08 , and an average distance of 0.71 ± 0.38 mm.  The qualitative evaluation suggest that with pre-training the network can learn to generalize better and provide refined and more complete segmentations in comparison to incomplete annotations.",False,images/ByL_UAjsG
H1NHbZ3jM,Adel Zakirov-Matvey Ezhov-Maxim Gusarev-Vladimir Alexandrovsky-Evgeny Shumilov,End-to-end dental pathology detection in 3D cone-beam computed tomography images,"Cone-beam computed tomography (CBCT) is valuable imaging method in dental diagnostics that provides information not available in traditional 2D imaging. However, interpretation of CBCT images is time-consuming process that requires physician to work with complicated software. In this work we propose an automated pipeline composed of several deep convolutional neural networks and algorithmic heuristics. Our task is two-fold: a) find locations of each present tooth inside 3D image volume, and b) detect several common tooth conditions in each tooth. Proposed system achieves 96.3% accuracy in tooth localization and average of 0.94 ROC AUC for 6 common tooth conditions.",False,images/H1NHbZ3jM
rycG7Zhof,Jonas Teuwen-Sil van de Leemput-Albert Gubern-Mérida-Alejandro Rodriguez-Ruiz-Ritse Mann-Babak Ehteshami Bejnordi,Soft tissue lesion detection in mammography using deep neural networks for object detection,"Computer-aided detection or decision support systems aim to improve breast cancer screening programs by helping radiologists to evaluate digital mammography (DM) exams. Commonly, such systems proceed in two steps: selection of candidate regions, and subsequent false positive reduction of the candidates as either suspicious lesions or inconspicuous breast tissue. In this study, we present a method based on deep learning for automatic detection of soft tissue lesions in DM using a one-step approach. A database of DM exams (mostly bilateral and two views) was collected from our institutional archive. In total, 7192 DM exams (23405 DM images) were acquired with systems from three different vendors (General Electric, Siemens, Hologic), of which 2883 contained malignant lesions verified with histopathology. The performance of our automated detection system was assessed using the free receiver operating characteristic (FROC) analysis. A maximum sensitivity of 0.97 at 3.56 false positives (FP) per image was achieved. The best model achieved a sensitivity of 0.73, 0.45, 0.31 at 0.1, 0.02 and 0.01 FP per image, respectively. Overall, the results of our evaluation suggests that our soft tissue lesion detection system can replace current two stage detectors.",False,images/rycG7Zhof
rJccgk3iM,Grzegorz Chlebus-Andrea Schenk-Jan Hendrik Moltz-Bram van Ginneken-Horst Karl Hahn-Hans Meine,Deep learning based automatic liver tumor segmentation in CT with shape-based post-processing,"Accurate automatic liver tumor segmentation would have a big impact on liver therapy planning procedures and follow-up reporting, thanks to automation, standardization and incorporation of full volumetric information. In this work, we propose a fully automatic method for liver tumor segmentation in CT images based on a 2D convolutional deep neural network with a shape-based post-processing. We ran our experiments on the LiTS dataset and evaluated detection and segmentation performance. Our proposed method achieves segmentation quality for detected lesions comparable to a human expert and is able to detect 77% of potentially measurable tumor lesions according to the RECIST 1.1 guidelines. We submitted our results to the LiTS challenge achieving state-of-the-art performance.",False,images/rJccgk3iM
HyifSbnif,Mohammed Safwan-Sai Saketh Chennamsetty-Avinash Kori-Varghese Alex-Ganapathy Krishnamurthi,Classification of Breast Cancer and Grading of Diabetic Retinopathy & Macular Edema using Ensemble of Pre-trained Convolutional Neural Networks,"In this manuscript, we present a deep learning based approach for detection and classification of medical conditions such as classification of breast cancer and grading of diabetic retinopathy and macular edema. The performance of a convolutional neural network is dependent on the architecture of the network, amount of training data and data pre-processing. Transfer learning is oft utilized in deep learning so as to counter the limited availability of high quality annotated data. Hence, we create an ensemble of pre-trained classifiers by making use of models with different topologies and data normalization schemes. In general, the variance associated with an ensemble of classifiers is lower compared to a single classifier and thus generalizes better on unseen data. An F1 score based model pruning technique was utilized for deciding the optimal number of classifiers in the ensemble. The proposed technique was tested on two separate biomedical image challenges, namely the (1) classification of breast cancer from histology images [BACH-2018] and (2) grading of diabetic retinopathy and macular edema from fundus images [IDRiD-2018]. 
On the histology data, our technique was adjudged jointly as the top performing algorithm while for the task of diabetic retinopathy grading, the technique was declared as the 4th best performing algorithm.",False,images/HyifSbnif
ByNCjAjsz,Mr Luke Greenwood-Mr Maged Habib-Professor Andrew Hunter-Dr Bashir Al-Diri,Automated Segmentation of Cystic Macular Edema in OCT B-Scan Images,"The  analysis  of  retinal  Spectral  Domain  Optical  Coherence  Tomography  (SD-
OCT) images by trained medical professionals can be used to provide useful in-
sights into many various diseases. It is the most popular method of retinal imaging
due to it’s non invasive nature and the useful information it provides for making an
accurate diagnosis. In this paper, we present a deep learning approach for the au-
tomating the segmentation of cystic macular edema (fluid) in retinal OCT B-Scan
images. Our network makes use of atrous convolutions, skip connections, weight
decay and significant image augmentation to ensure the most accurate segmenta-
tion result possible without the need for any features to be manually constructed.
The network is evaluated against a publicly available dataset and achieved a max-
imal Dice coefficient of 95.2%, thus making it the current best performer on that
dataset.",False,images/ByNCjAjsz
HJ1RffhjM,Wei Dai-Nanqing Dong-Zeya Wang-Xiaodan Liang-Hao Zhang-Eric P. Xing,SCAN: Structure Correcting Adversarial Network for Organ Segmentation in Chest X-rays,"Chest X-ray (CXR) is one of the most commonly prescribed medical imaging procedures, often with over 2–10x more scans than other imaging modalities. These voluminous CXR scans place significant workloads on radiologists and medical practitioners. Organ segmentation is a key step towards effective computer- aided detection on CXR. In this work, we propose Structure Correcting Adversarial Network (SCAN) to segment lung fields and the heart in CXR images. SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities inherent in human physiology. Specifically, the critic network learns the higher order structures in the masks in order to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network. Through an adversarial process, the critic network guides the segmentation network to achieve more realistic segmentation that mimics the ground truth. Extensive evaluation shows that our method produces highly accurate and realistic segmentation. Using only very limited training data available, our model reaches human-level performance without relying on any pre- trained model. Our method surpasses the current state-of-the-art and generalizes well to CXR images from different patient populations and disease profiles.",False,images/HJ1RffhjM
SkHVVZniz,Sil C. van de Leemput-Mathias Prokop-Bram van Ginneken-Rashindra Manniesing,Stacked Bidirectional Convolutional LSTMs for 3D Non-contrast CT Reconstruction from Spatiotemporal 4D CT,"The imaging workup in acute stroke can be simplified by reconstructing the non-contrast CT (NCCT) from CT perfusion (CTP) images, resulting in reduced workup time and radiation dose. This work presents a stacked bidirectional convolutional LSTM (C-LSTM) network to predict 3D volumes from 4D spatiotemporal data. Several parameterizations of the C-LSTM network were trained on a set of 17 CTP-NCCT pairs to learn to reconstruct NCCT from CTP and were subsequently quantitatively evaluated on a separate cohort of 16 cases. The results show that C-LSTM network clearly outperforms basic reconstruction methods and provides a promising general deep learning approach for handling high-dimensional spatiotemporal medical data.",True,images/SkHVVZniz
B1V7k-2sz,Sahin Olut-Yusuf Huseyin Sahin-Ugur Demir-Gozde Unal,Generative Adversarial Training for MRA Image Synthesis Using Multi-Contrast MRI,"Magnetic Resonance Angiography (MRA) has become an essential MR contrast for
imaging and evaluation of vascular anatomy and related diseases. MRA acquisitions
are typically ordered for vascular interventions, whereas in typical scenarios, MRA
sequences can be absent in the patient scans. This motivates the need for a technique
that generates inexistent MRA from existing MR multi-contrast, which could be
a valuable tool in retrospective subject evaluations and imaging studies. In this
paper, we present a generative adversarial network (GAN) based technique to
generate MRA from T1-weighted and T2-weighted MRI images, for the first time
to our knowledge. To better model the representation of vessels which the MRA
inherently highlights, we design a loss term dedicated to a faithful reproduction of
vascularities. To that end, we incorporate steerable filter responses of the generated
and reference images inside a Huber function loss term. Extending the well-
established generator-discriminator architecture based on the recent PatchGAN
model with the addition of steerable filter loss, the proposed steerable GAN (sGAN)
method is evaluated on the large public database IXI. Experimental results show
that the sGAN outperforms the baseline GAN method in terms of an overlap score
with similar PSNR values, while it leads to improved visual perceptual quality.",True,images/B1V7k-2sz
S11Xr-3iM,Thomas Joyce-Agisilaos Chartsias-Sotirios A. Tsaftaris,Deep Multi-Class Segmentation Without Ground-Truth Labels,"In this paper we demonstrate that through the use of adversarial training and additional unsupervised costs it is possible to train a multi-class anatomical segmentation algorithm without any ground-truth labels for the data set to be segmented. Specifically, using labels from a different data set of the same anatomy (although potentially in a different modality) we train a model to synthesise realistic multi-channel label masks from input cardiac images in both CT and MRI, through adversarial learning. However, as is to be expected, generating realistic mask images is not, on its own, sufficient for the segmentation task: the model can use the input image as a source of noise and synthesise highly realistic segmentation masks that do no necessarily correspond spatially to the input. To overcome this, we introduce additional unsupervised costs, and demonstrate that these provide sufficient further guidance to produce good segmentation results. We test our proposed method on both CT and MR data from the multi-modal whole heart segmentation challenge (MM-WHS) [1], and show the effect of our unsupervised costs on improving the segmentation results, in comparison to a variant without them.

[1] MICCAI 2017 MM-WHS Challenge. http://www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs17/. Accessed: 2018-04-11.",True,images/S11Xr-3iM
rJevSbniM,Irina Sanchez-Veronica Vilaplana,Brain MRI super-resolution using 3D generative adversarial networks,"In this work we propose an adversarial learning approach to generate high resolution MRI scans from low resolution images. The architecture, based on the SRGAN model, adopts 3D convolutions to exploit volumetric information. For the discriminator, the adversarial loss uses least squares in order to stabilize the training. For the generator, the loss function is a combination of a least squares adversarial loss and a content term based on mean square error and image gradients in order to improve the quality of the generated images. We explore different solutions for the upsampling phase. We present promising results that improve classical interpolation, showing the potential of the approach for 3D medical imaging super-resolution.",True,images/rJevSbniM
H1hWfZnjM,Ben A Duffy-Wenlu Zhang-Haoteng Tang-Lu Zhao-Meng Law-Arthur W Toga-Hosung Kim,Retrospective correction of motion artifact affected structural MRI images using deep learning of simulated motion,"Head motion during MRI acquisition presents significant problems for subsequent neuroimaging analyses. In this work, we propose to use convolutional neural networks (CNNs) to correct motion-corrupted images as well as investigate a possible improvement by augmenting L1 loss with adversarial loss. For training, in order to gain access to a ground-truth, we first selected a large number of motion-free images from the ABIDE dataset. We then added simulated motion artifacts on these images to produce motion corrupted data and a 3D regression CNN was trained to predict the motion-free volume as the output. We tested the CNN on unseen simulated data as well as real motion affected data. Quantitative evaluation was carried out using metrics such as Structural Similarity (SSIM) index, Correlation Coefficient (CC), and Tissue Contrast T-score (TCT). It was found that Gaussian smoothing as a conventional method did not significantly differ in SSIM, CC and RMSE from the uncorrected data. On the other hand, the two CNN models successfully removed the motion-related artifact as their SSIM and CC significantly increased after their correction and the error was reduced. The CNN displayed significantly larger TCT compared to the uncorrected images whereas the adversarial network, while improved did not show a significantly increased TCT, which may be explained also by its over-enhancement of edges. Our results suggest that the proposed CNN framework enables the network to generalize well to both unseen simulated motion artifacts as well as real motion artifact-affected data. The proposed method could easily be adapted to estimate a motion severity score, which could be used as a score of quality control or as a nuisance covariate in subsequent statistical analyses.",True,images/H1hWfZnjM
HJK6goojf,Nelson Martins-Eva Costa-Diana Veiga-Manuel Ferreira-Miguel Coimbra,Joint Capsule Segmentation in Ultrasound Images of the Metacarpophalangeal Joint using Convolutional Neural Networks,"This work addresses the automatic segmentation of the joint capsule in ultrasound images of the metacarpophalangeal joint using an adapted version of the well known UNet model. These images are used in the diagnosis of rheumatic diseases, one of the main causes of impairment and pain in developed countries. The identification of the joint capsule gives important clues about the presence or Rheumatoid Arthritis. This structure can be used to extract metrics to help quantify the disease stage and progression. The solution proposed here has the potential to reduce the burden on the radiologists as well as the subjectivity of the diagnosis by providing quantitative measurements, such as the synovitis area. The proposed approach was compared with two other works present in the literature. Results show that our solution outperforms the two reference methods with 90% of the joint capsules identified with a DICE higher than 0.67.",False,images/HJK6goojf
HkgCl-3if,Zhang Lin-Karani Neerav-Tanner Christine-Konukoglu Ender,Temporal Interpolation via Motion Field Prediction,"Navigated 2D multi-slice dynamic Magnetic Resonance (MR) imaging enables high contrast 4D MR imaging during free breathing and provides in-vivo observations for treatment planning and guidance. Navigator slices are vital for retrospective stacking of 2D data slices in this method. However, they also prolong the acquisition sessions. Temporal interpolation of navigator slices can be used to reduce the number of navigator acquisitions without degrading specificity in stacking. In this work, we propose a convolutional neural network (CNN) based method for temporal interpolation via motion field prediction. The proposed formulation incorporates the prior knowledge that a motion field underlies changes in the image intensities over time. Previous approaches that interpolate directly in the intensity space are prone to produce blurry images or even remove structures in the images. Our method avoids such problems and faithfully preserves the information in the image. Further, an important advantage of our formulation is that it provides an unsupervised estimation of bi-directional motion fields. We show that these motion fields can be used to halve the number of registrations required during 4D reconstruction, thus substantially reducing the reconstruction time.",True,images/HkgCl-3if
HyGqw13jM,Boheng Zhang-Shenglei Huang-Shaohan Hu,Multi-scale Neural Networks for Retinal Blood Vessels Segmentation,"Existing supervised approaches didn’t make use of the low-level features which are actually effective to semantic segmentation. And another deficiency is that they didn’t consider the relation between pixels, which means effective features are not extracted. In this paper, we applied semantic segmentation on retinal blood vessel images and proposed a novel convolutional neural network which make sufficient use of low-level features together with high-level features and involves atrous convolution to get multi-scale features which should be considered as effective features. Our model is tested on three standard benchmarks - DRIVE, STARE, and CHASE databases. The results presents that our model significantly outperforms existing approaches in terms of accuracy, sensitivity, specificity, the area under the ROC curve and the highest prediction speed. Our work provides evidence of the power of wide and deep neural networks in retinal blood vessels segmentation task which could be applied on other medical images tasks.",False,images/HyGqw13jM
Hy9NHaijf,Yu-An Chung-Wei-Hung Weng,Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval,"Deep neural networks have been investigated in learning latent representations of medical images, yet most of the studies limit their approach in a single supervised convolutional neural network (CNN), which usually rely heavily on a large scale annotated dataset for training. To learn image representations with less supervision involved, we propose a deep Siamese CNN (SCNN) architecture that can be trained with only binary image pair information. We evaluated the learned image representations on a task of content-based medical image retrieval using a publicly available multiclass diabetic retinopathy fundus image dataset. The experimental results show that our proposed deep SCNN is comparable to the state-of-the-art single supervised CNN, and requires much less supervision for training.",False,images/Hy9NHaijf
Sk_P2Q9sG,Yongchan Kwon-Joong-Ho Won-Beom Joon Kim-Myunghee Cho Paik,Uncertainty quantification using Bayesian neural networks in classification: Application to ischemic stroke lesion segmentation,"Most recent research of neural networks in the field of computer vision has focused on improving accuracy of point predictions by developing various network architectures or learning algorithms. Uncertainty quantification accompanied by point estimation can lead to a more informed decision, and the quality of prediction can be improved. In medical imaging applications, assessment of uncertainty could potentially reduce untoward outcomes due to suboptimal decisions. In this paper, we invoke a Bayesian neural network and propose a natural way to quantify uncertainty in classification problems by decomposing predictive uncertainty into two parts, aleatoric and epistemic uncertainty. The proposed method takes into account discrete nature of the outcome, yielding correct interpretation of each uncertainty. We demonstrate that the proposed uncertainty quantification method provides additional insight to the point prediction using images from the Ischemic Stroke Lesion Segmentation Challenge.",True,images/Sk_P2Q9sG
Hks1TRisM,Martin Rajchl-Nick Pawlowski-Daniel Rueckert-Paul M. Matthews-Ben Glocker,NeuroNet: Fast and Robust Reproduction of Multiple Brain Image Segmentation Pipelines,"NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study that have been automatically segmented into brain tissue and cortical and sub-cortical structures using the standard neuroimaging pipelines. Training a single model from these complementary and partially overlapping label maps yields a new powerful ``""all-in-one"", multi-output segmentation tool. The processing time for a single subject is reduced by an order of magnitude compared to running each individual software package. We demonstrate very good reproducibility of the original outputs while increasing robustness to variations in the input data. We believe NeuroNet could be an important tool in large-scale population imaging studies and serve as a new standard in neuroscience by reducing the risk of introducing bias when choosing a specific software package.",True,images/Hks1TRisM
BJenxxhof,Majd Zreik-Robbert W. van Hamersvelt-Jelmer M. Wolterink-Tim Leiner-Max A. Viergever-Ivana Isgum,Automatic Detection and Characterization of Coronary Artery Plaque and Stenosis using a Recurrent Convolutional Neural Network in Coronary CT Angiography,"Different types of atherosclerotic plaque and varying grades of stenosis lead to different management of patients with obstructive coronary artery disease. Therefore, it is crucial to determine the presence and classify the type of coronary artery plaque, as well as to determine the presence and the degree of a stenosis.
The study includes consecutively acquired coronary CT angiography (CCTA) scans of 131 patients. In these, presence and plaque type in the coronary arteries (no plaque, non-calcified, mixed, calcified) as well as presence and anatomical significance of coronary stenosis (no stenosis, non-significant, significant) were manually annotated by identifying the start and end points of the fragment of the artery affected by the plaque. To perform automatic analysis, a multi-task recurrent convolutional neural network is utilized. The network uses CCTA and coronary artery centerline as its inputs, and extracts features from the region defined along the coronary artery centerline using a 3D convolutional neural network. Subsequently, the extracted features are used by a recurrent neural network that performs two simultaneous multi-label classification tasks. In the first task, the network detects and characterizes the type of the coronary artery plaque. In the second task, the network detects and determines the anatomical significance of the coronary artery stenosis.
The results demonstrate that automatic characterization of coronary artery plaque and stenosis with high accuracy and reliability is feasible. This may enable automated triage of patients to those without coronary plaque, and those with coronary plaque and stenosis in need for further cardiovascular workup.",True,images/BJenxxhof
HkBtaBjoz,Tanja Elss-Hannes Nickisch-Tobias Wissel-Rolf Bippus-Michael Morlock-Michael Grass,Motion Estimation in Coronary CT Angiography Images using Convolutional Neural Networks,"Coronary CT angiography has become a preferred technique for the detection and diagnosis of coronary artery disease, but image artifacts due to cardiac motion frequently interfere with evaluation. Several motion compensation approaches have been developed which deal with motion estimation based on 3-D/3-D registration of multiple heart phases. The scan range required for multi-phase reconstruction is a limitation in clinical practice. In this paper, the feasibility of single-phase, image-based motion estimation by convolutional neural networks (CNNs) is investigated. First, the required data for supervised learning is generated by a forward model which introduces simulated axial motion to artifact-free CT cases. Second, regression networks are trained to estimate underlying 2D motion vectors from axial coronary cross-sections. In a phantom study with computer-simulated vessels, CNNs predict the motion direction and the motion strength with average accuracies of 1.08° and 0.06 mm, respectively. Motivated by these results, clinical performance is evaluated based on twelve prospectively ECG-triggered clinical cases and achieves average accuracies of 20.66° and 0.94 mm. Transferability and generalization capabilities are demonstrated by motion estimation and subsequent compensation on six clinical cases with real cardiac motion artifacts.",True,images/HkBtaBjoz
rkKvBAiiz,Guillem Cucurull-Konrad Wagstyl-Arantxa Casanova-Petar Veličković-Estrid Jakobsen-Michal Drozdzal-Adriana Romero-Alan Evans-Yoshua Bengio,Convolutional neural networks for mesh-based parcellation of the cerebral cortex,"In order to understand the organization of the cerebral cortex, it is necessary to create a map or parcellation of cortical areas. Reconstructions of the cortical surface created from structural MRI scans, are frequently used in neuroimaging as a common coordinate space for representing multimodal neuroimaging data. These meshes are used to investigate healthy brain organization as well as abnormalities in neurological and psychiatric conditions. We frame cerebral cortex parcellation as a mesh segmentation task, and address it by taking advantage of recent advances in generalizing convolutions to the graph domain. In particular, we propose to assess graph convolutional networks and graph attention networks, which, in contrast to previous mesh parcellation models, exploit the underlying structure of the data to make predictions. We show experimentally on the Human Connectome Project dataset that the proposed graph convolutional models outperform current state-of-the-art and baselines, highlighting the potential and applicability of these methods to tackle neuroimaging challenges, paving the road towards a better characterization of brain diseases.",True,images/rkKvBAiiz
S1NnlZnjG,Nikolas Lessmann-Bram van Ginneken-Pim A. de Jong-Ivana Isgum,Iterative fully convolutional neural networks for automatic vertebra segmentation,"Precise segmentation of the vertebrae is often required for automatic detection of vertebral abnormalities. This especially enables incidental detection of abnormalities such as compression fractures in images that were acquired for other diagnostic purposes. While many CT and MR scans of the chest and abdomen cover a section of the spine, they often do not cover the entire spine. Additionally, the first and last visible vertebrae are likely only partially included in such scans. In this paper, we therefore approach vertebra segmentation as an instance segmentation problem. A fully convolutional neural network is combined with an instance memory that retains information about already segmented vertebrae. This network iteratively analyzes image patches, using the instance memory to search for and segment the first not yet segmented vertebra. At the same time, each vertebra is classified as completely or partially visible, so that partially visible vertebrae can be excluded from further analyses. We evaluated this method on spine CT scans from a vertebra segmentation challenge and on low-dose chest CT scans. The method achieved an average Dice score of 95.8% and 92.1%, respectively, and a mean absolute surface distance of 0.194 mm and 0.344 mm.",True,images/S1NnlZnjG
r1malb3jz,Julia M. H. Noothout-Bob D. de Vos-Jelmer M. Wolterink-Tim Leiner-Ivana Išgum,CNN-based Landmark Detection in Cardiac CTA Scans,"Fast and accurate anatomical landmark detection can benefit many medical image
analysis methods. Here, we propose a method to automatically detect anatomical
landmarks in medical images.
Automatic landmark detection is performed with a patch-based fully convolutional
neural network (FCNN) that combines regression and classification. For any given
image patch, regression is used to predict the 3D displacement vector from the
image patch to the landmark. Simultaneously, classification is used to identify
patches that contain the landmark. Under the assumption that patches close to a
landmark can determine the landmark location more precisely than patches farther
from it, only those patches that contain the landmark according to classification
are used to determine the landmark location. The landmark location is obtained by
calculating the average landmark location using the computed 3D displacement
vectors.
The method is evaluated using detection of six clinically relevant landmarks in
coronary CT angiography (CCTA) scans : the right and left ostium, the bifurcation
of the left main coronary artery (LM) into the left anterior descending and the left
circumflex artery, and the origin of the right, non-coronary, and left aortic valve
commissure. The proposed method achieved an average Euclidean distance error
of 2.19 mm and 2.88 mm for the right and left ostium respectively, 3.78 mm for the
bifurcation of the LM, and 1.82 mm, 2.10 mm and 1.89 mm for the origin of the
right, non-coronary, and left aortic valve commissure respectively, demonstrating
accurate performance.
The proposed combination of regression and classification can be used to accurately
detect landmarks in CCTA scans.",True,images/r1malb3jz
H1sdHFiif,Marysia Winkels-Taco S. Cohen,3D G-CNNs for Pulmonary Nodule Detection,"    Convolutional Neural Networks (CNNs) require a large amount of annotated data to learn from, which is often difficult to obtain in the medical domain. In this paper we show that the sample complexity of CNNs can be significantly improved by using 3D roto-translation group convolutions (G-Convs) instead of the more conventional translational convolutions. These 3D G-CNNs were applied to the problem of false positive reduction for pulmonary nodule detection, and proved to be substantially more effective in terms of performance, sensitivity to malignant nodules, and speed of convergence compared to a strong and comparable baseline architecture with regular convolutions, data augmentation and a similar number of parameters. For every dataset size tested, the G-CNN achieved a FROC score close to the CNN trained on ten times more data.",True,images/H1sdHFiif
SJwod1hjz,Xi Wang-Hao Chen-Caixia Gan-Huangjing Lin-Qi Dou-Qitao Huang-Muyan Cai-Pheng-Ann Heng,Weakly Supervised Learning for Whole Slide Lung Cancer Image Classification,"Histopathology image analysis serves as the gold standard for diagnosis of cancer and is directly related to the subsequent therapeutic treatment. However, pixel-wise delineated annotations on whole slide images (WSIs) are time-consuming and tedious, which poses difficulties in building a large-scale training dataset. How to effectively utilize available whole slide image-level label, which can be easily acquired, for deep learning is quite appealing. The main barrier on this task is due to the heterogeneous patterns in fine magnification level but only the WSI-level labels are provided. Furthermore, a gigapixel scale WSI can not be easily analysed due to the immeasurable computational cost. In this paper, we propose a weakly supervised approach for fast and effective classification on whole slide lung cancer images. Our method takes advantage of a patch-based fully convolutional network for discriminative block retrieval. Furthermore, context-aware feature selection and aggregation strategies are proposed to generate globally holistic WSI descriptor. Extensive experiments demonstrate that our method outperforms state-of-the-art methods by a large margin with accuracy of 97.1%. In addition, we highlight that a small number of available coarse annotations can contribute to further accuracy improvement. We believe that deep learning has great potential to assist pathologists for histology image diagnosis in the near future.",True,images/SJwod1hjz
BJPMK-3jM,Sharmin Pathan-Yi Hong,Predictive Image Regression for Longitudinal Studies with Missing Data,"In this paper, we propose a predictive regression model for longitudinal images with missing data based on large deformation diffeomorphic metric mapping (LDDMM) and deep neural networks. Instead of directly predicting image scans, our model predicts a vector momentum sequence associated with a baseline image. This momentum sequence parameterizes the original image sequence in the LDDMM framework and lies in the tangent space of the baseline image, which is Euclidean. A recurrent network with long term-short memory (LSTM) units encodes the time-varying changes in the vector-momentum sequence, and a convolutional neural network (CNN) encodes the baseline image of the vector momenta. Features extracted by the LSTM and CNN are fed into a decoder network to reconstruct the vector momentum sequence, which is used for the image sequence prediction by deforming the baseline image with LDDMM shooting. To handle the missing images at some time points, we adopt a binary mask to ignore their reconstructions in the loss calculation. We evaluate our model on synthetically generated images and the brain MRIs from the OASIS dataset. Experimental results demonstrate the promising predictions of the spatiotemporal changes in both datasets, irrespective of large or subtle changes in longitudinal image sequences.",True,images/BJPMK-3jM
By47mM_oG,Xin Yi-Scott Adams-Paul Babyn-Abdul Elnajmi,Automatic catheter detection in pediatric X-ray images using a scale-recurrent network and synthetic data,"Catheters are commonly inserted life supporting devices. X-ray images are used to assess the position of a catheter immediately after placement as serious complications can arise from malpositioned catheters. Previous computer vision approaches to detect catheters on X-ray images either relied on low-level cues that are not sufficiently robust or only capable of processing a limited number or type of catheters. With the resurgence of deep learning, supervised training approaches are begining to showing promising results. However, dense annotation maps are required, and the work of a human annotator is hard to scale. In this work, we proposed a simple way of synthesizing catheters on X-ray images and a scale recurrent network for catheter detection. By training on adult chest X-rays, the proposed network exhibits promising detection results on pediatric chest/abdomen X-rays in terms of both precision and recall.",True,images/By47mM_oG
SkjdxkhoG,Farhad G. Zanjani-Svitlana Zinger-Babak E. Bejnordi-Jeroen AWM van der Laak-Peter H.N. de With,Histopathology Stain-Color Normalization Using Deep Generative Models,"Performance of designed CAD algorithms for histopathology image analysis is affected by the amount of variations in the samples such as color and intensity of stained images. Stain-color normalization is a well-studied technique for compensating such effects at the input of CAD systems. In this paper, we introduce unsupervised generative neural networks for performing stain-color normalization. For color normalization in stained hematoxylin and eosin (H&E) images, we present three methods based on three frameworks for deep generative models: variational auto-encoder (VAE), generative adversarial networks (GAN) and deep convolutional Gaussian mixture models (DCGMM). Our contribution is defining the color normalization as a learning generative model that is able to generate various color copies of the input image through a nonlinear parametric transformation. In contrast to earlier generative models proposed for stain-color normalization, our approach does not need any labels for data or any other assumptions about the H&E image content. Furthermore, our models learn a parametric transformation during training and can convert the color information of an input image to resemble any arbitrary reference image. This property is essential in time-critical CAD systems in case of changing the reference image, since our approach does not need retraining in contrast to other proposed generative models for stain-color normalization. Experiments on histopathological H&E images with high staining variations, collected from different laboratories, show that our proposed models outperform quantitatively state-of-the-art methods in the measure of color constancy with at least 10-15%, while the converted images are visually in agreement with this performance improvement.",True,images/SkjdxkhoG
HyzbMW3oz,Agnieszka Barbara Szczotka-Daniele Ravi-Dzhoshkun Ismail Shakir-Stephen P Pereira-Tom Vercauteren,Learning from Irregularly Sampled Data with Deep Nadaraya–Watson Kernel Regression Networks (NWNet): Application to Endomicroscopy Image Reconstruction,"Probe-based Confocal Laser Endomicroscopy (pCLE) enables more accurate diagnosis via optical biopsy. pCLE probes relay on of fibres bundles, which generate irregularly sampled signals. Current pCLE reconstruction is based on interpolating irregular signals onto an over-sampled Cartesian grid, using a sub-optimal Delaunay triangulation based linear interpolation scheme. High-quality reconstruction with improved information representation should be possible with the use of Deep Convolutional Neural Networks (CNNs). However, classical CNNs are limited to take as an input only Cartesian images, not irregular data. In this work, we propose to embed Nadaraya-Watson (NW) kernel regression into the CNN framework as a novel trainable CNN layer that allows for processing of irregularly sampled data represented as sparse data on a Cartesian grid. We design a new NWNet architecture in conjunction with examplar-based super-resolution CNN, which allows reconstructing high-quality pCLE images from the irregularly sampled input data. Models were trained on a database of 8806 images from 238 pCLE video sequences. The results were validated through an image quality assessment based on a composition of the following metrics: PSNR, SSIM, GCF. Our analysis indicates that the proposed solution unlocks the potential of CNNs for sparse data processing. NW layer is the main contribution of our end-to-end model performing pCLE image reconstruction directly from sparse imaging input to high-resolution cartesian images. Our method outperforms the reconstruction method in current clinical use.",False,images/HyzbMW3oz
rk0xLisiM,Zaneta Swiderska-Chadaj-Hans Pinckaers-Mart van Rijthoven-Maschenka Balkenhol-Margarita Melnikova-Oscar Geessink-Quirine Manson-Geert Litjens-Jeroen van der Laak-Francesco Ciompi,Convolutional Neural Networks for Lymphocyte detection in Immunohistochemically Stained Whole-Slide Images,"Recent advances in cancer immunotherapy have boosted the interest in the role played by the immune system in cancer treatment. In particular, the presence of tumor-infiltrating lymphocytes (TILs) have become a central research topic in oncology and pathology. Consequently, a method to automatically detect and quantify immune cells is of great interest. In this paper, we present a comparison of different deep learning (DL) techniques for the detection of lymphocytes in immunohistochemically stained (CD3 and CD8) slides of breast, prostate and colon cancer. The compared methods cover the state-of-the-art in object localization, classification and segmentation: Locality Sensitive Method (LSM), U-net, You Only Look Once (YOLO) and fully-convolutional networks (FCNN). A dataset with 109,841 annotated cells from 58 whole-slide images was used  for this study. Overall, U-net and YOLO achieved the highest results, with an  F1-score of 0.78 in regular tissue areas. U-net approach was more robust to biological and staining variability and could also handle staining and tissue artifacts.",True,images/rk0xLisiM
HkFV8g3oz,Christoph Baur-Shadi Albarqouni-Nassir Navab,MelanoGANs: High Resolution Skin Lesion Synthesis with GANs,"Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking images of faces, scenery and even medical images. Unfortunately, they usually require large training datasets, which are often scarce in the medical field, and to the best of our knowledge GANs have been only applied for medical image synthesis at fairly low resolution. However, many state-of-the-art machine learning models operate on high resolution data as such data carries indispensable, valuable information. In this work, we try to generate realistically looking high resolution images of skin lesions with GANs, using only a small training dataset of 2000 samples. The nature of the data allows us to do a direct comparison between the image statistics of the generated samples and the real dataset. We both quantitatively and qualitatively compare state-of-the-art GAN architectures such as DCGAN and LAPGAN against a modification of the latter for the task of image generation at a resolution of 256x256px. Our investigation shows that we can approximate the real data distribution with all of the models, but we notice major differences when visually rating sample realism, diversity and artifacts. In a set of use-case experiments on skin lesion classification, we further show that we can successfully tackle the problem of heavy class imbalance with the help of synthesized high resolution melanoma samples.",False,images/HkFV8g3oz
rJgE13soM,Simon Graham-Hao Chen-Qi Dou-Pheng-Ann Heng-Nasir Rajpoot,MILD-Net: Minimal Information Loss Dilated Network for Gland Instance Segmentation in Colon Histology Images,"The analysis of glandular morphology within colon histopathology images is a crucial step in determining the stage of colon cancer. Despite the importance of this task, manual segmentation is laborious, time-consuming and can suffer from subjectivity among pathologists. The rise of computational pathology has led to the development of automated methods for gland segmentation that aim to overcome the challenges of manual segmentation. However, this task is non-trivial due to the large variability in glandular appearance and the difficulty in differentiating between certain glandular and non-glandular histological structures. Furthermore, within pathological practice, a measure of uncertainty is essential for diagnostic decision making. For example, ambiguous areas may require further examination from numerous pathologists. To address these challenges, we propose a fully convolutional neural network that counters the loss of information caused by max-pooling by re-introducing the original image at multiple points within the network. We also use atrous spatial pyramid pooling with varying dilation rates for resolution maintenance and multi-level aggregation. To incorporate uncertainty, we introduce random transformations during test time for an enhanced segmentation result that simultaneously generates an uncertainty map, highlighting areas of ambiguity. We show that this map can be used to define a metric for disregarding predictions with high uncertainty. The proposed network achieves state-of-the-art performance on the GlaS challenge dataset, as part of MICCAI 2015, and on a second independent colorectal adenocarcinoma dataset.
",True,images/rJgE13soM
